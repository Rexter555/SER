{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import libraries \nimport librosa\nimport librosa.display\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom matplotlib.pyplot import specgram\nimport pandas as pd\nimport glob \nfrom sklearn.metrics import confusion_matrix\nimport IPython.display as ipd  # To play sound in the notebook\nimport os\nimport sys\nimport warnings\n# ignore warnings \nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\nTESS = \"/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\nRAV = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\nSAVEE = \"/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/\"\n\n# Run one example \ndir_list = os.listdir(SAVEE)\ndir_list[0:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the data location for SAVEE\ndir_list = os.listdir(SAVEE)\n\n# parse the filename to get the emotions\nemotion=[]\npath = []\nfor i in dir_list:\n    if i[-8:-6]=='_a':\n        emotion.append('male_angry')\n    elif i[-8:-6]=='_d':\n        emotion.append('male_disgust')\n    elif i[-8:-6]=='_f':\n        emotion.append('male_fear')\n    elif i[-8:-6]=='_h':\n        emotion.append('male_happy')\n    elif i[-8:-6]=='_n':\n        emotion.append('male_neutral')\n    elif i[-8:-6]=='sa':\n        emotion.append('male_sad')\n    elif i[-8:-6]=='su':\n        emotion.append('male_surprise')\n    else:\n        emotion.append('male_error') \n    path.append(SAVEE + i)\n    \n# Now check out the label count distribution \nSAVEE_df = pd.DataFrame(emotion, columns = ['labels'])\nSAVEE_df['source'] = 'SAVEE'\nSAVEE_df = pd.concat([SAVEE_df, pd.DataFrame(path, columns = ['path'])], axis = 1)\nSAVEE_df.labels.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use the well known Librosa library for this task \nfname = SAVEE + 'DC_f11.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets play a happy track\nfname = SAVEE + 'DC_h11.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dir_list = os.listdir(RAV)\ndir_list.sort()\n\nemotion = []\ngender = []\npath = []\nfor i in dir_list:\n    fname = os.listdir(RAV + i)\n    for f in fname:\n        part = f.split('.')[0].split('-')\n        emotion.append(int(part[2]))\n        temp = int(part[6])\n        if temp%2 == 0:\n            temp = \"female\"\n        else:\n            temp = \"male\"\n        gender.append(temp)\n        path.append(RAV + i + '/' + f)\n\n        \nRAV_df = pd.DataFrame(emotion)\nRAV_df = RAV_df.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'})\nRAV_df = pd.concat([pd.DataFrame(gender),RAV_df],axis=1)\nRAV_df.columns = ['gender','emotion']\nRAV_df['labels'] =RAV_df.gender + '_' + RAV_df.emotion\nRAV_df['source'] = 'RAVDESS'  \nRAV_df = pd.concat([RAV_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nRAV_df = RAV_df.drop(['gender', 'emotion'], axis=1)\nRAV_df.labels.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pick a fearful track\nfname = RAV + 'Actor_14/03-01-06-02-02-02-14.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pick a happy track\nfname = RAV + 'Actor_14/03-01-03-02-02-02-14.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dir_list = os.listdir(TESS)\ndir_list.sort()\ndir_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = []\nemotion = []\n\nfor i in dir_list:\n    fname = os.listdir(TESS + i)\n    for f in fname:\n        if i == 'OAF_angry' or i == 'YAF_angry':\n            emotion.append('female_angry')\n        elif i == 'OAF_disgust' or i == 'YAF_disgust':\n            emotion.append('female_disgust')\n        elif i == 'OAF_Fear' or i == 'YAF_fear':\n            emotion.append('female_fear')\n        elif i == 'OAF_happy' or i == 'YAF_happy':\n            emotion.append('female_happy')\n        elif i == 'OAF_neutral' or i == 'YAF_neutral':\n            emotion.append('female_neutral')                                \n        elif i == 'OAF_Pleasant_surprise' or i == 'YAF_pleasant_surprised':\n            emotion.append('female_surprise')               \n        elif i == 'OAF_Sad' or i == 'YAF_sad':\n            emotion.append('female_sad')\n        else:\n            emotion.append('Unknown')\n        path.append(TESS + i + \"/\" + f)\n\nTESS_df = pd.DataFrame(emotion, columns = ['labels'])\nTESS_df['source'] = 'TESS'\nTESS_df = pd.concat([TESS_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nTESS_df.labels.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets play a fearful track \nfname = TESS + 'YAF_fear/YAF_dog_fear.wav' \n\ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets play a happy track \nfname =  TESS + 'YAF_happy/YAF_dog_happy.wav' \n\ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EmotionData = pd.concat([SAVEE_df, RAV_df, TESS_df], axis = 0)\nprint(EmotionData.labels.value_counts())\n#EmotionData.head()\nEmotionData.to_csv(\"Data_path.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EmotionData.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets pick up the meta-data that we got from our first part of the Kernel\nref = pd.read_csv(\"Data_path.csv\")\nref.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**FEATURE EXTRACTION**\nThere are lots of features which we can get from an audio data such as \n* Spectral Centroid\n* Zero Crossing Rate\n* Chroma Frequencies\n* Mel Frequency Ceptral Coefficient(MFCC)\n* Spectral Roll off\nBut for human voice characterization and modelling MFCC is the best feature so that's why we are using MFCC feature and extract it for each of the data.","metadata":{}},{"cell_type":"markdown","source":"Lets extract MFCC feature for one of the example voice","metadata":{}},{"cell_type":"code","source":"# Source - RAVDESS; Gender - Male; Emotion - Happy \npath = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_11/03-01-03-01-02-02-11.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n\n# audio wave\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.waveplot(X, sr=sample_rate)\nplt.title('Audio sampled at 44100 hrz')\n\n# MFCC\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\n#Here we are displaying Spectrogram for the Happy voice and lets visualiza how its look like\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()\n\nipd.Audio(path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now lets extract this feature for entire dataset and then concatinate this feature column into our dataframe","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(columns=['feature'])\n\n# loop feature extraction over the entire dataset\ncounter=0\nfor index,path in enumerate(ref.path):\n    X, sample_rate = librosa.load(path\n                                  , res_type='kaiser_fast'\n                                  ,duration=2.5\n                                  ,sr=44100\n                                  ,offset=0.5\n                                 )\n    sample_rate = np.array(sample_rate)\n    \n    # mean as the feature. Could do min and max etc as well. \n    mfccs = np.mean(librosa.feature.mfcc(y=X, \n                                        sr=sample_rate, \n                                        n_mfcc=13),\n                    axis=0)\n    df.loc[counter] = [mfccs]\n    counter=counter+1   \n\n# Check a few records to make sure its processed successfully\nprint(len(df))\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#concatinating the feature column into the complete dataframe\ndf = pd.concat([ref,pd.DataFrame(df['feature'].values.tolist())],axis=1)\ndf[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace NA with 0\ndf=df.fillna(0)\nprint(df.shape)\ndf[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils import np_utils, to_categorical\nfrom sklearn.preprocessing import LabelEncoder\nimport pickle","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\nfrom keras import regularizers\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model, model_from_json\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\nfrom keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\nfrom keras.utils import np_utils, to_categorical\nfrom keras.callbacks import ModelCheckpoint","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation**\n\nHere we found that Accuracy is arround 0.9901 and loss is 0.0291 \nBut When we look around Validation set we found that val_loss: 1.0878  val_acc: 0.7398\nWhich signifies the overfitting as val_loss>train_loss","metadata":{}},{"cell_type":"markdown","source":"**To reduche the overfitting**\nWe require to improve the dataset: For that we will use data augmentation\nNext we will use 2D convNet or any other model to train our data","metadata":{}},{"cell_type":"code","source":"# Save model and weights\nmodel_name = 'Emotion_Model.h5'\nsave_dir = os.path.join(os.getcwd(), 'saved_models')\n\nif not os.path.isdir(save_dir):\n    os.makedirs(save_dir)\nmodel_path = os.path.join(save_dir, model_name)\nmodel.save(model_path)\nprint('Save model and weights at %s ' % model_path)\n\n# Save the model to disk\nmodel_json = model.to_json()\nwith open(\"model_json.json\", \"w\") as json_file:\n    json_file.write(model_json)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading json and model architecture \njson_file = open('model_json.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n\n# load weights into new model\nloaded_model.load_weights(\"saved_models/Emotion_Model.h5\")\nprint(\"Loaded model from disk\")\n \n# Keras optimiser\nopt = keras.optimizers.Adam(lr=0.0001)\nloaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\nscore = loaded_model.evaluate(X_test, y_test, verbose=0)\nprint(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = loaded_model.predict(X_test, \n                         batch_size=10, \n                         verbose=1)\n\npreds=preds.argmax(axis=1)\npreds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predictions \npreds = preds.astype(int).flatten()\npreds = (lb.inverse_transform((preds)))\npreds = pd.DataFrame({'predictedvalues': preds})\n\n# Actual labels\nactual=y_test.argmax(axis=1)\nactual = actual.astype(int).flatten()\nactual = (lb.inverse_transform((actual)))\nactual = pd.DataFrame({'actualvalues': actual})\n\n# Lets combined both of them into a single dataframe\nfinaldf = actual.join(preds)\nfinaldf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"finaldf.to_csv('Predictions.csv', index=False)\nfinaldf.groupby('predictedvalues').count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = finaldf.actualvalues.unique()\nclasses.sort()    \nprint(classification_report(finaldf.actualvalues, finaldf.predictedvalues, target_names=classes))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n    df_cm = pd.DataFrame(\n        confusion_matrix, index=class_names, columns=class_names, \n    )\n    fig = plt.figure(figsize=figsize)\n    try:\n        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n    except ValueError:\n        raise ValueError(\"Confusion matrix values must be integers.\")\n        \n    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Gender recode function\ndef gender(row):\n    if row == 'female_disgust' or 'female_fear' or 'female_happy' or 'female_sad' or 'female_surprise' or 'female_neutral':\n        return 'female'\n    elif row == 'male_angry' or 'male_fear' or 'male_happy' or 'male_sad' or 'male_surprise' or 'male_neutral' or 'male_disgust':\n        return 'male'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the predictions file \nfinaldf = pd.read_csv(\"Predictions.csv\")\nclasses = finaldf.actualvalues.unique()\nclasses.sort()    \n\n# Confusion matrix \nc = confusion_matrix(finaldf.actualvalues, finaldf.predictedvalues)\nprint(accuracy_score(finaldf.actualvalues, finaldf.predictedvalues))\nprint_confusion_matrix(c, class_names = classes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's see how we can reduce the wrong prediction i.e False Positive and False Negatives","metadata":{}},{"cell_type":"markdown","source":"# Data Augmentation\nLets add augmented data to our Dataset\nBefore proceeding lets understand some of the datasets","metadata":{}},{"cell_type":"code","source":"# Augmentation methods\n#########################\ndef noise(data):\n    \"\"\"\n    Adding White Noise.\n    \"\"\"\n    # you can take any distribution from https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.random.html\n    noise_amp = 0.05*np.random.uniform()*np.amax(data)   # more noise reduce the value to 0.5\n    data = data.astype('float64') + noise_amp * np.random.normal(size=data.shape[0])\n    return data\n    \ndef shift(data):\n    \"\"\"\n    Random Shifting.\n    \"\"\"\n    s_range = int(np.random.uniform(low=-5, high = 5)*1000)  #default at 500\n    return np.roll(data, s_range)\n    \ndef stretch(data, rate=0.8):\n    \"\"\"\n    Streching the Sound. Note that this expands the dataset slightly\n    \"\"\"\n    data = librosa.effects.time_stretch(data, rate)\n    return data\n    \ndef pitch(data, sample_rate):\n    \"\"\"\n    Pitch Tuning.\n    \"\"\"\n    bins_per_octave = 12\n    pitch_pm = 2\n    pitch_change =  pitch_pm * 2*(np.random.uniform())   \n    data = librosa.effects.pitch_shift(data.astype('float64'), \n                                      sample_rate, n_steps=pitch_change, \n                                      bins_per_octave=bins_per_octave)\n    return data\n    \ndef dyn_change(data):\n    \"\"\"\n    Random Value Change.\n    \"\"\"\n    dyn_change = np.random.uniform(low=-0.5 ,high=7)  # default low = 1.5, high = 3\n    return (data * dyn_change)\n    \ndef speedNpitch(data):\n    \"\"\"\n    peed and Pitch Tuning.\n    \"\"\"\n    # you can change low and high here\n    length_change = np.random.uniform(low=0.8, high = 1)\n    speed_fac = 1.2  / length_change # try changing 1.0 to 2.0 ... =D\n    tmp = np.interp(np.arange(0,len(data),speed_fac),np.arange(0,len(data)),data)\n    minlen = min(data.shape[0], tmp.shape[0])\n    data *= 0\n    data[0:minlen] = tmp[0:minlen]\n    return data\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use one audio file in previous parts again\nfname = '/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/JK_f11.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Play it again to refresh our memory\nipd.Audio(data, rate=sampling_rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now add some augmentation into the voice","metadata":{}},{"cell_type":"code","source":"#Static Noise\nx = noise(data)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(x, sr=sampling_rate)\nipd.Audio(x, rate=sampling_rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#shift: It basically add more time and shift the voice a little bit\nx = shift(data)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(x, sr=sampling_rate)\nipd.Audio(x, rate=sampling_rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Stretch: It basically slow down the speaking time \nx = stretch(data)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(x, sr=sampling_rate)\nipd.Audio(x, rate=sampling_rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#It adds more pitch into the voice\nx = pitch(data, sampling_rate)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(x, sr=sampling_rate)\nipd.Audio(x, rate=sampling_rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dynamic Change\nx = dyn_change(data)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(x, sr=sampling_rate)\nipd.Audio(x, rate=sampling_rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Speed and Pitch: This one is basically the combination of both speed and pitch\nx = speedNpitch(data)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(x, sr=sampling_rate)\nipd.Audio(x, rate=sampling_rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now choosing our meta data and augment the voice into the whole datasets\nref.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I am going to use Stretch and Dynamic change as other methods may reduce the accuracy.\nIf we add pitch or speed and pitch it will change the voice quality and seems like a female is taking althoug its a male's voice.","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Note this takes a couple of minutes (~16 mins) as we're iterating over 4 datasets, and with augmentation  \ndf = pd.DataFrame(columns=['feature'])\ndf_dyn_change = pd.DataFrame(columns=['feature'])\ndf_stretch = pd.DataFrame(columns=['feature'])\ncnt = 0\n\n# loop feature extraction over the entire dataset\nfor i in tqdm(ref.path):\n    \n    # first load the audio \n    X, sample_rate = librosa.load(i\n                                  , res_type='kaiser_fast'\n                                  ,duration=2.5\n                                  ,sr=44100\n                                  ,offset=0.5\n                                 )\n\n    # take mfcc and mean as the feature. Could do min and max etc as well. \n    mfccs = np.mean(librosa.feature.mfcc(y=X, \n                                        sr=np.array(sample_rate), \n                                        n_mfcc=13),\n                    axis=0)\n    \n    df.loc[cnt] = [mfccs]   \n\n\n    # dyn_change \n    aug = dyn_change(X)\n    aug = np.mean(librosa.feature.mfcc(y=aug, \n                                    sr=np.array(sample_rate), \n                                    n_mfcc=13),    \n                  axis=0)\n    df_dyn_change.loc[cnt] = [aug]\n\n    # stretch\n    aug = stretch(X)\n    aug = np.mean(librosa.feature.mfcc(y=aug, \n                                    sr=np.array(sample_rate), \n                                    n_mfcc=13),    \n                  axis=0)\n    df_stretch.loc[cnt] = [aug]   \n\n    cnt += 1\n\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Combining the feature to the dataset\ndf = pd.concat([ref,pd.DataFrame(df['feature'].values.tolist())],axis=1)\ndf_dyn_change = pd.concat([ref,pd.DataFrame(df_dyn_change['feature'].values.tolist())],axis=1)\ndf_stretch = pd.concat([ref,pd.DataFrame(df_stretch['feature'].values.tolist())],axis=1)\nprint(df.shape,df_dyn_change.shape,df_stretch.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([df,df_dyn_change,df_stretch],axis=0,sort=False)\ndf=df.fillna(0)\ndel df_dyn_change,df_stretch\n\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split between train and test \nX_train, X_test, y_train, y_test = train_test_split(df.drop(['path','labels','source'],axis=1)\n                                                    , df.labels\n                                                    , test_size=0.25\n                                                    , shuffle=True\n                                                    , random_state=42\n                                                   )\n\n# Lets see how the data present itself before normalisation \nX_train[150:160]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets do data normalization \nmean = np.mean(X_train, axis=0)\nstd = np.std(X_train, axis=0)\n\nX_train = (X_train - mean)/std\nX_test = (X_test - mean)/std\n\n# Check the dataset now \nX_train[150:160]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets few preparation steps to get it into the correct format for Keras \nX_train = np.array(X_train)\ny_train = np.array(y_train)\nX_test = np.array(X_test)\ny_test = np.array(y_test)\n\n# one hot encode the target \nlb = LabelEncoder()\ny_train = np_utils.to_categorical(lb.fit_transform(y_train))\ny_test = np_utils.to_categorical(lb.fit_transform(y_test))\n\nprint(X_train.shape)\nprint(lb.classes_)\n\n# Pickel the lb object for future use \nfilename = 'labels'\noutfile = open(filename,'wb')\npickle.dump(lb,outfile)\noutfile.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = np.expand_dims(X_train, axis=2)\nX_test = np.expand_dims(X_test, axis=2)\nX_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import (Convolution2D, GlobalAveragePooling2D, BatchNormalization, Flatten, Dropout,\n                          GlobalMaxPool2D, MaxPool2D, concatenate, Activation, Input, Dense)\nimport keras\nfrom keras import regularizers\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model, model_from_json\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\nfrom keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\nfrom keras.utils import np_utils, to_categorical\nfrom keras.callbacks import (EarlyStopping, LearningRateScheduler,\n                             ModelCheckpoint, TensorBoard, ReduceLROnPlateau)\nfrom keras import losses, models, optimizers\nfrom keras.activations import relu, softmax","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n1. Data Augmentation method   \n'''\ndef speedNpitch(data):\n    \"\"\"\n    Speed and Pitch Tuning.\n    \"\"\"\n    # you can change low and high here\n    length_change = np.random.uniform(low=0.8, high = 1)\n    speed_fac = 1.2  / length_change # try changing 1.0 to 2.0 ... =D\n    tmp = np.interp(np.arange(0,len(data),speed_fac),np.arange(0,len(data)),data)\n    minlen = min(data.shape[0], tmp.shape[0])\n    data *= 0\n    data[0:minlen] = tmp[0:minlen]\n    return data\n\n'''\n2. Extracting the MFCC feature as an image (Matrix format).  \n'''\ndef prepare_data(df, n, aug, mfcc):\n    X = np.empty(shape=(df.shape[0], n, 216, 1))\n    input_length = sampling_rate * audio_duration\n    \n    cnt = 0\n    for fname in tqdm(df.path):\n        file_path = fname\n        data, _ = librosa.load(file_path, sr=sampling_rate\n                               ,res_type=\"kaiser_fast\"\n                               ,duration=2.5\n                               ,offset=0.5\n                              )\n\n        # Random offset / Padding\n        if len(data) > input_length:\n            max_offset = len(data) - input_length\n            offset = np.random.randint(max_offset)\n            data = data[offset:(input_length+offset)]\n        else:\n            if input_length > len(data):\n                max_offset = input_length - len(data)\n                offset = np.random.randint(max_offset)\n            else:\n                offset = 0\n            data = np.pad(data, (offset, int(input_length) - len(data) - offset), \"constant\")\n\n        # Augmentation? \n        if aug == 1:\n            data = speedNpitch(data)\n        \n        # which feature?\n        if mfcc == 1:\n            # MFCC extraction \n            MFCC = librosa.feature.mfcc(data, sr=sampling_rate, n_mfcc=n_mfcc)\n            MFCC = np.expand_dims(MFCC, axis=-1)\n            X[cnt,] = MFCC\n            \n        else:\n            # Log-melspectogram\n            melspec = librosa.feature.melspectrogram(data, n_mels = n_melspec)   \n            logspec = librosa.amplitude_to_db(melspec)\n            logspec = np.expand_dims(logspec, axis=-1)\n            X[cnt,] = logspec\n            \n        cnt += 1\n    \n    return X\n\n\n'''\n3. Confusion matrix plot \n'''        \ndef print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n    '''Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n\n    Arguments\n    ---------\n    confusion_matrix: numpy.ndarray\n        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n        Similarly constructed ndarrays can also be used.\n    class_names: list\n        An ordered list of class names, in the order they index the given confusion matrix.\n    figsize: tuple\n        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n        the second determining the vertical size. Defaults to (10,7).\n    fontsize: int\n        Font size for axes labels. Defaults to 14.\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        The resulting confusion matrix figure\n    '''\n    df_cm = pd.DataFrame(\n        confusion_matrix, index=class_names, columns=class_names, \n    )\n    fig = plt.figure(figsize=figsize)\n    try:\n        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n    except ValueError:\n        raise ValueError(\"Confusion matrix values must be integers.\")\n\n    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n    \n    \n'''\n# 4. Create the 2D CNN model \n'''\ndef get_2d_conv_model(n):\n    ''' Create a standard deep 2D convolutional neural network'''\n    nclass = 14\n    inp = Input(shape=(n,216,1))  #2D matrix of 30 MFCC bands by 216 audio length.\n    x = Convolution2D(32, (4,10), padding=\"same\")(inp)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = MaxPool2D()(x)\n    x = Dropout(rate=0.2)(x)\n    \n    x = Convolution2D(32, (4,10), padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = MaxPool2D()(x)\n    x = Dropout(rate=0.2)(x)\n    \n    x = Convolution2D(32, (4,10), padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = MaxPool2D()(x)\n    x = Dropout(rate=0.2)(x)\n    \n    x = Convolution2D(32, (4,10), padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = MaxPool2D()(x)\n    x = Dropout(rate=0.2)(x)\n    \n    x = Flatten()(x)\n    x = Dense(64)(x)\n    x = Dropout(rate=0.2)(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = Dropout(rate=0.2)(x)\n    \n    out = Dense(nclass, activation=softmax)(x)\n    model = models.Model(inputs=inp, outputs=out)\n    \n    opt = optimizers.Adam(0.001)\n    model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['acc'])\n    return model\n\n'''\n# 5. Other functions \n'''\nclass get_results:\n    '''\n    We're going to create a class (blueprint template) for generating the results based on the various model approaches. \n    So instead of repeating the functions each time, we assign the results into on object with its associated variables \n    depending on each combination:\n        1) MFCC with no augmentation  \n        2) MFCC with augmentation \n        3) Logmelspec with no augmentation \n        4) Logmelspec with augmentation\n    '''\n    \n    def __init__(self, model_history, model ,X_test, y_test, labels):\n        self.model_history = model_history\n        self.model = model\n        self.X_test = X_test\n        self.y_test = y_test             \n        self.labels = labels\n\n    def create_plot(self, model_history):\n        '''Check the logloss of both train and validation, make sure they are close and have plateau'''\n        plt.plot(model_history.history['loss'])\n        plt.plot(model_history.history['val_loss'])\n        plt.title('model loss')\n        plt.ylabel('loss')\n        plt.xlabel('epoch')\n        plt.legend(['train', 'test'], loc='upper left')\n        plt.show()\n\n    def create_results(self, model):\n        '''predict on test set and get accuracy results'''\n        opt = optimizers.Adam(0.001)\n        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n        score = model.evaluate(X_test, y_test, verbose=0)\n        print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n\n    def confusion_results(self, X_test, y_test, labels, model):\n        '''plot confusion matrix results'''\n        preds = model.predict(X_test, \n                                 batch_size=16, \n                                 verbose=2)\n        preds=preds.argmax(axis=1)\n        preds = preds.astype(int).flatten()\n        preds = (lb.inverse_transform((preds)))\n\n        actual = y_test.argmax(axis=1)\n        actual = actual.astype(int).flatten()\n        actual = (lb.inverse_transform((actual)))\n\n        classes = labels\n        classes.sort()    \n\n        c = confusion_matrix(actual, preds)\n        print_confusion_matrix(c, class_names = classes)\n    \n    def accuracy_results_gender(self, X_test, y_test, labels, model):\n        '''Print out the accuracy score and confusion matrix heat map of the Gender classification results'''\n    \n        preds = model.predict(X_test, \n                         batch_size=16, \n                         verbose=2)\n        preds=preds.argmax(axis=1)\n        preds = preds.astype(int).flatten()\n        preds = (lb.inverse_transform((preds)))\n\n        actual = y_test.argmax(axis=1)\n        actual = actual.astype(int).flatten()\n        actual = (lb.inverse_transform((actual)))\n        \n        # print(accuracy_score(actual, preds))\n        \n        actual = pd.DataFrame(actual).replace({'female_angry':'female'\n                   , 'female_disgust':'female'\n                   , 'female_fear':'female'\n                   , 'female_happy':'female'\n                   , 'female_sad':'female'\n                   , 'female_surprise':'female'\n                   , 'female_neutral':'female'\n                   , 'male_angry':'male'\n                   , 'male_fear':'male'\n                   , 'male_happy':'male'\n                   , 'male_sad':'male'\n                   , 'male_surprise':'male'\n                   , 'male_neutral':'male'\n                   , 'male_disgust':'male'\n                  })\n        preds = pd.DataFrame(preds).replace({'female_angry':'female'\n               , 'female_disgust':'female'\n               , 'female_fear':'female'\n               , 'female_happy':'female'\n               , 'female_sad':'female'\n               , 'female_surprise':'female'\n               , 'female_neutral':'female'\n               , 'male_angry':'male'\n               , 'male_fear':'male'\n               , 'male_happy':'male'\n               , 'male_sad':'male'\n               , 'male_surprise':'male'\n               , 'male_neutral':'male'\n               , 'male_disgust':'male'\n              })\n\n        classes = actual.loc[:,0].unique() \n        classes.sort()    \n\n        c = confusion_matrix(actual, preds)\n        print(accuracy_score(actual, preds))\n        print_confusion_matrix(c, class_names = classes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ref.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MFCC without augmentation","metadata":{}},{"cell_type":"code","source":"sampling_rate=44100\naudio_duration=2.5\nn_mfcc = 30\nmfcc = prepare_data(ref, n = n_mfcc, aug = 0, mfcc = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(mfcc\n                                                    , ref.labels\n                                                    , test_size=0.25\n                                                    , shuffle=True\n                                                    , random_state=42\n                                                   )\n\n\n# one hot encode the target \nlb = LabelEncoder()\ny_train = np_utils.to_categorical(lb.fit_transform(y_train))\ny_test = np_utils.to_categorical(lb.fit_transform(y_test))\n\n# Normalization as per the standard NN process\nmean = np.mean(X_train, axis=0)\nstd = np.std(X_train, axis=0)\n\nX_train = (X_train - mean)/std\nX_test = (X_test - mean)/std\n\n# Build CNN model \nmodel1 = get_2d_conv_model(n=n_mfcc)\nmodel1_history = model1.fit(X_train, y_train, validation_data=(X_test, y_test), \n                    batch_size=16, verbose = 1, epochs=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = get_results(model1_history,model,X_test,y_test, ref.labels.unique())\nresults.create_plot(model1_history)\nresults.create_results(model1)\nresults.confusion_results(X_test, y_test, ref.labels.unique(), model1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that after applying 2D ConvNet we may got less accuracy but our validation loss increases drastically as well as our model is converging.","metadata":{}},{"cell_type":"markdown","source":"# MFCC with Augmentation","metadata":{}},{"cell_type":"code","source":"sampling_rate=44100\naudio_duration=2.5\nn_mfcc = 30\nmfcc_aug = prepare_data(ref, n = n_mfcc, aug = 1, mfcc = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split between train and test \nX_train, X_test, y_train, y_test = train_test_split(mfcc_aug\n                                                    , ref.labels\n                                                    , test_size=0.25\n                                                    , shuffle=True\n                                                    , random_state=42\n                                                   )\n\n# one hot encode the target \nlb = LabelEncoder()\ny_train = np_utils.to_categorical(lb.fit_transform(y_train))\ny_test = np_utils.to_categorical(lb.fit_transform(y_test))\n\n# Normalization as per the standard NN process\n# mean = np.mean(X_train, axis=0)\n# std = np.std(X_train, axis=0)\n\n# X_train = (X_train - mean)/std\n# X_test = (X_test - mean)/std\n\n# Build CNN model \nmodel2 = get_2d_conv_model(n=n_mfcc)\nmodel2_history = model2.fit(X_train, y_train, validation_data=(X_test, y_test), \n                    batch_size=16, verbose = 2, epochs=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = get_results(model2_history,model2,X_test,y_test, ref.labels.unique())\nresults.create_plot(model2_history)\nresults.create_results(model2)\nresults.confusion_results(X_test, y_test, ref.labels.unique(), model2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n**From the above observation we found that after adding augmented data to the dataset and we slighltly improve the dataset which is quite around 83% and validation accuracy is around 76% **","metadata":{}},{"cell_type":"markdown","source":"From all the models which is used to train we found that **MFCC without augmentation** higher precision \nAccuracy=85%\nvalidation Accuracy=81%\nso we are going to save that model","metadata":{}},{"cell_type":"code","source":"#Save weights to HDF5\nfilepath=\"Emotion_Model2.h5\"\nmodel1.save(filepath)\n\n#Serialize to json format\nmodel1_json=model1.to_json()\nwith open(\"model1.json\",\"w\") as json_file:\n    json_file.write(model1_json)\nprint(\"Saved model to disk\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loading json and create model\njson_file=open(\"model1.json\",\"r\")\nloaded_model_json=json_file.read()\njson_file.close()\nloaded_model=model_from_json(loaded_model_json)\n#Load weights into new model\nloaded_model.load_weights(\"Emotion_Model2.h5\")\nprint(\"Loaded Model from disk\")\n\n#Evaluate Model\nloaded_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nscore=loaded_model.evaluate(X_test, y_test, verbose=0)\nprint(loaded_model.metrics_names[1],score[1]*100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction on new data","metadata":{}},{"cell_type":"code","source":"newData,newSR= librosa.load(\"/kaggle/input/my-data/Recording.wav\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is my voice that I recorded and converted into .wav format","metadata":{}},{"cell_type":"code","source":"ipd.Audio(\"/kaggle/input/vnaudio/VNAR.wav\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(newData, sr=newSR)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets transform the dataset so we can apply the predictions\nnewData, newSR = librosa.load(\"/kaggle/input/my-data/Recording.wav\"\n                              ,duration=2.5\n                              ,sr=44100\n                              ,offset=0.5)\n\nnewSR = np.array(newSR)\nmfccs = np.mean(librosa.feature.mfcc(y=newData, sr=newSR, n_mfcc=13),axis=0)\nnewdf = pd.DataFrame(data=mfccs).T\nnewdf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newdf= np.expand_dims(newdf,axis=2)\nprint(newdf.shape)\nnewpred=model.predict(newdf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = filename = '/kaggle/input/labels/labels'\ninfile = open(filename,'rb')\nlb = pickle.load(infile)\ninfile.close()\n\n# Get the final predicted label\nfinal = newpred.argmax(axis=1)\nfinal = final.astype(int).flatten()\nfinal = (lb.inverse_transform((final)))\nprint(final) #emo(final) #gender(final) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}